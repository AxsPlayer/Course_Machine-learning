# Note of Course 2.

## Different kind of NLP representations.
- Denotation: Every word is a denotation.
- Distributional similarity based representations: You can get a lot of value by representing a word by means of its neighbors.

## Attention.
- Word vector representation.
For the same word, the representation vectors are different, in terms of as center word, compared to as context word.

## Questions.
- Q1: Is the Object function J(theta) is the mathematical formula format of Neural Network?
- Q2: How to update context word matrix in Neural Network?
- Q3: Would the Object function find the similar words in context for target word, which does not make any sense, for example, cat dog cat?