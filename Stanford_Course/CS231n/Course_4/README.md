# Note of Course 4.(Introduction to Neural Networks)

## Gradient descent.
- Numerical gradient: slow but easy to write.
- Analytic gradient: fast but error-prone.
- Thus, the numerical gradient would always be used to check the correctness of the analytic gradient result.

## Image weight templates.
- The weight of Neural network is the template of images.
- And the preceding weight matrix in the upper layers, are the weighted sum of different kind of templates.

## Activation function.
- The ReLU activation function is the most similar function to the neurons.
- Different choice of activation functions: Sigmoid, tanh, ReLU, Leaky ReLU, Maxout, ELU.