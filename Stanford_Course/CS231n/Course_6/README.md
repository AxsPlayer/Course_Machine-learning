# Note of Course 6.(Training Neural Networks)

## Activation Function.
- Sigmoid: 
	- If the input is too positive or too negative, the gradient would be near to zero.
	- Sigmoid outputs are not zero-centered
	- Exp() is a bit compute expensive
- The input should not always be positive or negative, or the gradient will be always in the same direction and can not converge quickly.
- tanh:
	- Squashes numbers to range [-1, 1]
	- zero centered(nice)
	- still kills gradients when saturated
	- a bit better than sigmoid but also has a lot of problems
- ReLU(Rectified Linear Unit):
	- often used in Neural Network.
	- does not saturate (in positive region)
	- very computationally effiecient
	- converges much faster
	- more biology plausible than sigmoid
	- but also not zero-centered output
	- but also an annoyance

	- The ReLU unit will be dead if the weight initialization is not good at first time or the learning rate is set too high that after overupdating the weight.
- Leaky ReLU:
	- Does not saturate
	- Computationally efficient
	- Converges much faster than sigmoid/tanh
	- will not die
- PReLU: add the parameter to the Leaky ReLU.
- ELU: Exponential Linear Units
	- All benefits of ReLU
	- Colser to zero mean outputs
	- Negative saturation regime compared with Leaky ReLU
	- Adds some robustness to noise
	- Computation needs exp()
- Maxout Neuron: generalize ReLU and Leaky ReLU, but double the parameters.
- In practice:
	- Use ReLU. Be careful with your learning rates.
	- Try out Leaky ReLU/ Maxout/ELU
	- Try out tanh but don't expect much
	- don't use sigmoid
